\chapter{Experiments}
\section{Amount of help text}
We use a maximum of 4 lines of text from the help text attribute while reading the xml files of tools. This attribute is noisy and contains text which is not useful to be set up as a basis for finding similarity. We need to be careful of the amount of text which we extract from this attribute. Experimentally, we verify that using $4$ lines of text from this source works better in most of the cases. The results become worse if we do not use this attribute at all.

\section{One and two sources instead of three}
We have used three different attributes for compiling the collection of tokens. These attributes are different from each other. Using them together to make one set of tokens for each tool would not be beneficial. Instead of combining the tokens from these attributes, we compute similarities using tokens from these different attributes and then combine them using an optimization technique by learning optimal weights on each attribute.

\section{Similarity measures}
For calculating similarity scores using input and output file types, we use the jaccard index because we do not intend to learn any "concept" hidden in a set of file types for a tool. For similarities computed for name and description and help text, we use cosine similarity. Both these similarity measures give a real number between $0.0$ and $1.0$ as a similarity score for a pair of documents (tools).

\section{Latent Semantic Analysis}
Using latent semantic analysis, we learn dense vector representation for a document. It reduces the rank of the document-tokens matrix. We need to find out this reduction factor which can improve the similarity scores and find relevant similar tools among tools compared to using full-rank document-tokens matrix. We follow an approach of lowering the rank by certain factors and look at how the values in these matrices are spread. We reduce the rank of documents-tokens matrices to $70\%$, $50\%$, $30\%$, $10\%$ and $5\%$ of the full-rank value. Moreover, we also verify the similarity matrices corresponding to these low-rank matrices. We expect the documents-tokens and similarity matrices to be more dense compared to no rank reduction. To reduce the ranks, we consider documents-tokens matrices of name and description and help text and leave the input and output matrix in its full-rank state. We use singular value decomposition method from $numpy\'s$ linear algebra package. 

\section{Paragraph Vector}
In this approach, we learn a fixed-length dense vector for each document. We set the dimensions of these document vectors. When the size of a document is lower, we use a lower number of dimensions for computing the fixed-length vector. For example, in figure 5 we see that the number of tokens is higher for help text compared to name and description. To learn fixed-length vectors, we set the vector's length to be $50$ for name and description and $300$ for help text. Here as well, we learn paragraph vectors only for name and description and help text and not for input and output file types. We use $gensim\'s$ model to learn these paragraph vectors.

\subsection{Distributed bag-of-words}
Out of the two approaches to learn paragraph vectors, we use distributed bag-of-words approach to learn vectors for each document. It does not compute word vectors and relies only on paragraph vectors to predict the randomly chosen words from a sampled text window. Learning only the paragraph vectors is faster and computationally less expensive. 

\section{Gradient Descent}
To optimize the similarity scores from computed from multiple attributes, we use gradient descent optimizer to learn weights on the scores of these attributes. Based on the similarity measures, we define an error function for the optimizer which iteratively minimizes it.

\subsection{Learning rates}
We use time-based decay to set learning rate for each iteration. It starts off a higher value of $0.05$ and gradually decreases over iterations. The drop in the learning rate is smooth, neither too steep not too flat. We set a maximum iteration of 100 within which the learning saturates comfortably. When we start with $0.1$ or higher, there is a risk of optimizer divergence. If we start with $0.001$, the learning does not saturate within $100$ iterations (the learning becomes slow). 

\subsection{Code repositories}
The codebase used for this study is at github. There are separate branches for the different ideas discussed here. For latent semantic analysis approach, we have two branches - one uses full-rank documents-tokens matrices\footnote{\url{https://github.com/anuprulez/similar_galaxy_tools/tree/lsi}} and another uses documents-tokens matrices reduced to $5\%$ of the ful-rank\footnote{\url{https://github.com/anuprulez/similar_galaxy_tools/tree/lsi_005}}. Both these branches differ only in thier ranks of documents-tokens matrices. There is a separate branch for paragraph vectors approach\footnote{\url{https://github.com/anuprulez/similar_galaxy_tools/tree/doc2vec}}. All these code repositories are under MIT License\footnote{\url{https://github.com/anuprulez/similar_galaxy_tools/blob/master/LICENSE.md}}.