\chapter{Experiments}
\section{Amount of help text}
We use a maximum of 4 lines of text from the help text attribute while reading the xml files of tools. This attribute is noisy and contains text which is not useful to be set up as a basis for finding similarity. We need to be careful of the amount of text which we extract from this attribute. Experimently, we verify that using $4$ lines of text from this source works better in most of the cases.

\section{One and two sources instead of three}
We have used three different attributes for compiling the collection of tokens. These attributes are different from each other. Using them together to make one set of tokens for each tool would not be beneficial. We compute similarities for these different sources and combine them using optimization by learning optimal weights on each attribute.

\section{Similarity measures}
For finding similarity using input and output file types, we use jaccard index because we do not need to learn any "concept" hidden in a set of file types for a tool. For similarities computed for name and description and help text, we use cosine similarity. Both these similarity measures give a real number between $0.0$ and $1.0$ as a similarity score.

\section{Latent Semantic Analysis}
Using latent semantic analysis, we learn dense vector representation for a document. It reduces the rank of the document-tokens matrix. We need to find out this reduction factor which can improve the similarity scores among tools compared to using full-rank document-tokens matrix. We follow an approach of lowering the rank by certain factors and look at how the values in these matrices are spread. We reduce the rank to $70\%$, $50\%$, $30\%$, $10\%$ and $5\%$ of the full-rank value. Moreover, we also verify the similarity matrices corresponding to these low-rank matrices representations. We expect the documents-tokens and similarity matrices to be more dense compared to no rank reduction. To reduce the ranks, we consider documents-tokens matrices of name and description and help text and leave the input and output matrix in its full-rank state. We singular value decomposition routine from $numpy\'s$ linear algebra package. 

\section{Paragraph Vector}
In this approach, we learn a fixed-length dense vector representations for each document. We set the dimensions of these vectors. When the size of a document is lower, we use a lower number for computing the fixed-length vector. For example, in figure 5 we see that the number of tokens is higher for help text compared to name and description. To learn fixed-length vectors, we set the vector's length to be $50$ for name and description and $300$ for help text. Here as well, we learn paragraph vectors only for name and description and help text and not for input and output file types. We use $gensim\'s$ model to learn these paragraph vectors.

\subsection{Distributed bag-of-words}
We use this approach to learn vectors for each document. It does not use word-vectors and rely on paragraph vectors to predict the randomly chosen words from a sampled text window. Learning only the paragraph vectors is faster and computationally less expensive. 

\section{Gradient Descent}
To optimize the similarity scores from coming from multiple attributes, we use gradient descent optimizer to learn weights on the scores of these attributes. Based on the similarity measures, we construct a error function to the optimizer which minimizes it.

\subsection{Learning rates}
We use backtracking line search to set learning rate for each iteration. It expedites the learning by computing a learning rate that sufficiently diminishes the error function in each iteration of gradient descent. We set a maximum iteration to 100 within which the learning saturates when the learning rate start with a value of 0.1. When we start with 0.01 or smaller, the optimization does not converge within 100 iterations.
