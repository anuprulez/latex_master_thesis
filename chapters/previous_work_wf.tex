\chapter{Related work}\label{chap:previous_work_wf}

The paths we extract from workflows are sequential where tools are connected one after another. In order to predict the next tools, we should take into account all the previous connected tools. As our data possesses long chains of tools, we explore some literature from machine learning research which work on similar data. Interestingly, learning from sequential data is a popular task in many other fields like natural language processing \cite{0001KYS17, LiQYL16}. They use deep learning models to learn sequences of words which aims to classify sentiments in text, learn part-of-speech tagging and unique dense vector for each word. The classification of sentiments involves learning core contexts present in the chain of words. The part-of-speech tagging which divides a sentence into multiple parts of speech like adjective, adverb, conjunctions also depends on finding context hidden in the long chains of words. For sentiment analysis \cite{0001KYS17} achieves an accuracy of $\approx 85\%$ using recurrent neural network (gated recurrent unit). For part-of-speech tagging, the accuracy goes upto $\approx 93\%$. 

For clinical data as well, learning long sequences of data proves to be beneficial \cite{LiptonKEW15}. In this work, health states of patients recorded at different point of times are analysed by accessing their electronic health records. Using the learned model, the authors predict the future health states of a patient using the sequence of his/her health states in the past. The authors use long-short term memory (LSTM), a kind of recurrent neural network, to classify the sequences of health states. They achieve an accuracy of $\approx 85\%$ after applying necessary regularization techniques like dropout and weight normalization.

\cite{ChungGCB14, BoulangerICML2012} uses recurrent neural networks to model music and speech signals. The authors analyze the traditional recurrent and long-short term memory and gated recurrent units (GRU) and come to conclusion that the traditional recurrent units do not learn the semantics present in complete sequences while LSTM and GRU do. In this study, the authors learn sequences of 20 continuous samples (20 steps in time-series of speech) and predict the next 10 continuous samples (next 10 time steps). They also find out that GRU performs better than LSTM in terms of accuracy and running time. For musedata\footnote{\url{www.musedata.org}}, a collection of piano classical music, the performances (average of the negative log-likelihood) on test set noted by the authors are: GRU -5.99, LSTM - 6.23 and traditional recurrent units - 6.23. They test on 6 different types of musical and speech data and 4 out of these 6 types, GRU works better than the other two units.

These successful studies inspire us to avail benefits from the state-of-the-art sequential learning techniques like LSTM and GRU. They learn the tools connections inherent in the workflows to predict next possible tools at each time-step.

