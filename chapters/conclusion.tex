\chapter{Conclusion}
\section{Tools data}
The text for help text attribute was noisy containing lots of words which were generic and provided little information to categorize tools. On the other hand, it was necessary as it supplied more information about them and their functions. Due to the noisy nature of help text data, it needed more filtering and we considered only the first $4$ lines of text from this attribute. The data for the other attributes, input and output file types and name and description were helpful. The extraction of tools data from github's multiple repositories was slow\footnote{The extraction of $\approx$1050 tools took $\approx$ 30 minutes} and due to which we read information from the xml files of tools into our local tabular file and performed our analysis using that tabular file.

\section{Approaches}
We investigated two approaches to find similarity in Galaxy tools. The latent semantic analysis approach which relied on matrix rank reduction in order to remove unimportant concepts or dimensions worked better than using full-rank documents-token matrices. However, we were not sure about the quantity of reduction of noise dimensions and because of this, we ran the risk of losing important dimensions while reducing the noise. During optimization, it gave more importance to input and output file types which were many times undesirable. But, in general, this approach is simple and takes less time ($\approx350$ seconds for $\approx1050$ tools) to complete. We can easily compute the low-rank estimations of sparse matrices using singular value decomposition. 

The paragraph vectors approach worked better than the previous rank reduction technique in terms of finding similar tools in a robust way. The documents-tokens matrices it learned for name and description and help text were dense and encoded the semantic similarities among the documents. The documents which were similar in context learned similar vectors as proved by their dense similarity matrices. However, this approach is slow as it takes $\approx1000$ seconds to finish, most of which is taken to learn document vectors. We trained for $10$ epochs and each epoch had $800$ iterations to learn vectors for each document. Running for $200$ iterations over $10$ epochs did not fetch relevant results. Moreover, to specify the number of dimensions of paragraph vectors was an important concern. From figure 5, we saw that the average number of tokens for name and description was $\approx$ 2-3 times higher than that of help text. So, the vector dimensions for name and description attribute was set to a smaller number ($100$). 

The worst mean squared error is $1.0$. The paragraph vectors approach dropped the mean squared error to $\approx0.82$ while the latent semantic analysis approach could drop it only to $\approx0.92$ by reducing the rank of documents-tokens matrices to $5\%$.

\section{Optimization}
Learning the weights on the similarity scores from multiple attributes worked in a good way and we reached the saturation point already before $50^{th}$ iteration. We can reduce the number of iterations for gradient descent ($100$ to $\approx40$). We used gradient descent optimizer with mean squared error as loss function. We used mean squared error because we desired the hypothesis similarity scores distribution to be as close to the true distribution (based on similarity measures) as possible. We decreased the risk of getting stuck at saddle points or local minima by using momentum with an accelerated gradient to update the weight parameters.